<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval">
  <meta property="og:title" content="Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval"/>
  <meta property="og:description" content="https://lambdavi.github.io/select2plan/"/>
  <meta property="og:url" content="https://lambdavi.github.io/select2plan/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/1st_frame_all.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval">
  <meta name="twitter:description" content="https://lambdavi.github.io/select2plan/">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/1st_frame_all.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="select2plan vlm navigation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://lambdavi.github.io" target="_blank">Davide Buoso</a><sup>* 1,2</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Luke Robinson</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Giuseppe Averta</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Philip Torr</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Tim Franzmeyer</a><sup>† 2</sup>,</span>
                    <span class="author-block">
                    <a href="FIFTH AUTHOR PERSONAL LINK" target="_blank">Daniele De Martini</a><sup>† 2</sup>
                  </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Politecnico di Torino <sup>1</sup>, University of Oxford <sup>2</sup><br> ICRA 2025 (Under Review)</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author: davide.buoso@polito.it</small></span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Indicates Equal Supervision</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>
                  -->

                  <!-- Github link
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                -->
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted height="100%">
        <!-- Your video here -->
        <source src="static/videos/video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        High-level demonstration of S2P:the robot must reach the red mark from its location, controlled solely via the external camera, shown in the figure. S2P proposes candidate keypoints -- in yellow -- and draws them into the original image before requesting a feasible trajectory to an off-the-shelf VLM. The latter will output a trajectory -- green -- as a sequence of keypoints, ideally yielding a trajectory that avoids obstacles -- e.g. 3 and 9.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This study explores the potential of off-the-shelf Vision-Language Models (VLMs) for high-level robot planning in the context of autonomous navigation. Indeed, while most of existing learning-based approaches for path planning require extensive task-specific training/fine-tuning, we demonstrate how such training can be avoided for most practical cases. To do this, we introduce Select2Plan (S2P), a novel training-free frame work for high-level robot planning which completely eliminates the need for fine-tuning or specialised training. By leveraging structured Visual Question-Answering (VQA) and In-Context Learning (ICL), our approach drastically reduces the need for data collection, requiring a fraction of the task-specific data typically used by trained models, or even relying only on online data. Our method facilitates the effective use of a generally trained VLM in a flexible and cost-efficient way, and does not require additional sensing except for a simple monocular camera. We demonstrate its adaptability across various scene types, context sources, and sensing setups. We evaluate our approach in two distinct scenarios: traditional First-Person View (FPV) and infrastructure-driven Third-Person View (TPV) navigation, demonstrating the flexibility and simplicity of our method. Our technique significantly enhances the navigational capabilities of a baseline VLM of approximately 50% in TPV scenario, and is comparable to trained models in the FPV one, with as few as 20 demonstrations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero" style="padding-bottom: .75rem; padding-top: 1.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-bottom: .75rem; padding-top: 1.5rem;">
      <h3 class="title is-3">Can we plan and navigate in a training-free matter leveraging VQA and ICL?</h3>
      <p>
        <b>Select2Plan (S2P)</b> is a novel framework for high-level robot navigation that eliminates the need for extensive training or fine-tuning. The system leverages <b>off-the-shelf Vision-Language Models (VLMs)</b> combined with <b>Visual Question-Answering (VQA)</b> to make decisions without the need for specialized data collection.
      </p>
      <!--
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/what_can_we_learn.png" alt="" style="max-width: 768px; margin: auto;" />
      </figure>
      -->
      <br>
      <h3 class="title is-3">Can we adapt to different perspectives?</h3>
      <p>
        <b>S2P</b> adapts effortlessy to different navigation scenarios, including <b>First-Person View (FPV)</b> and <b>Third-Person View (TPV)</b>. This versatility enables it to operate in various conditions, from autonomous vehicle pathfinding to CCTV-based robotic control.<br>
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/1st_frame_all.png" alt="" style="max-width: 1328px; margin: auto;" />
      </figure>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;"></figure>
        <img src="static/images/3d_frame_all_i.png" alt="" style="max-width: 980px; margin: auto;" />
      </figure>
      <p>
        S2P incorporates <b>In-Context Learning (ICL)</b> combined with <b>experiential memory</b> to generate robust plans.<br>
        The memory retrieval system enables the robot to learn from previous experiences and generalize to new, unseen environments, improving navigation performance by as much as 50% without the need for additional training.
        <b>S2P performs comparably</b> to heavily trained models in traditional setups (FPV), achieving impressive results with significantly fewer data points, making it an efficient solution for high-level robot planning and navigation.
      </p>
      <br>
      <h4 class="title is-4">What matters in the context?</h4>
      <p>
        We also investigate how different data sources can affect the performance of S2P. In TPV scenario, we evaluate the performance of context coming from three different sources: same deployment scenario and same rooms, same scenario, different rooms
        same scenario, humans as demonstrators and finally, different scenario and different robotic support (online data).
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/context_v3.png" alt="" style="max-width: 768px; margin: auto;" />
      </figure>
    </div>
  </div>
</section>

<section class="section hero" style="padding-bottom: .75rem; padding-top: 1.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-bottom: .75rem; padding-top: 1.5rem;">
      <h3 class="title is-3">Results in FPV scenario</h3>
      <p>
        Our approach is called <b>EgoPack</b> and is composed of two stages. 
        In the first stage, a multi-task model is trained on a set of K known tasks. 
        In the second stage, the model is fine-tuned on a novel task using egopack’s cross-task interaction mechanism
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/table1.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
      </figure>

      <h3 class="title is-3">Results in TPV scenario</h3>
      <p>
        Our approach is called <b>EgoPack</b> and is composed of two stages. 
        In the first stage, a multi-task model is trained on a set of K known tasks. 
        In the second stage, the model is fine-tuned on a novel task using egopack’s cross-task interaction mechanism
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/table2.png" alt="" style="max-width: 420px; margin: auto; margin-bottom: 16px;" />
      </figure>

      <!--
      <h4 class="title is-4">Step 2: Novel Task Learning</h2>
        <div class="columns is-vcentered is-flex-center is-multiline">
          <div class="column is-half">
            <div class="text-block">
              <p>
                These heads model different and complementary perspectives on the content of the video. 
                We can collect these perspectives in a set of action-wise <b>task-specific prototypes</b>.
                <br><br>
                We call these prototypes a <b>backpack of skills</b> and they represent a frozen snapshot of what the 
                model has learnt in the pre-training phase.
              </p>
            </div>
          </div>
          <div class="column is-half">
            <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
              <img src="static/images/egopack.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
            </figure>
          </div>
          <div class="column is-half">
            <div class="text-block">
              <p>
                <ol>
                  <li>
                    When learning a novel task, we feed the temporal features through the 
                    task-specific heads of the pre-training tasks.
                  </li>
                  <br>
                  <li>
                    These features act as queries to look for the closest matching prototypes 
                    using k-NN in the features space.
                  </li>
                  <br>
                  <li>
                    We refine the task features using <b>Message Passing with task prototypes</b>.
                  </li>
                </ol>
                
              </p>
            </div>
          </div>
            <div class="column is-half">
              <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
                <img src="static/images/fusion.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
              </figure>
            </div>
        </div>
-->

    </div>
  </div>
</section>
<!--
<section class="section hero" style="padding-bottom: .75rem; padding-top: 1.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-bottom: .75rem; padding-top: 1.5rem;">
      <h3 class="title is-3">Experimental results</h3>
      <p>We validate EgoPack on Action Recognition (AR), Object State Change Classification (OSCC), Point of No Return (PNR) and Long Term Anticipation (LTA) from Ego4D.</p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/results_val.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
      </figure>
      <div class="columns is-vcentered is-flex-center is-multiline">
        <div class="column is-half">
          <div class="text-block">
            <h5 class="title is-5">Cross-tasks agreement ratio</h5>
            <p>
              How much different <i>perspectives</i> bring complementary information?
            </p>
          </div>
        </div>
        <div class="column is-half">
          <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
            <img src="static/images/viz_agreement.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
          </figure>
        </div>
        <div class="column is-half">
          <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
            <img src="static/images/queries.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
          </figure>
        </div>
        <div class="column is-half">
          <div class="text-block">
            <h5 class="title is-5" style="text-align: right;">Queried prototypes</h5>
            <p style="text-align: right;">
              When the novel task is OSCC, what are the closest prototypes from the AR and PNR tasks?
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


BibTex citation
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Cite us</h2>
      <pre><code>
@misc{buoso2024s2p,
  author    = {Peirone, Simone Alberto and Pistilli, Francesca and Alliegro, Antonio and Averta, Giuseppe},
  title     = {A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2024},
  pages     = {18275-18285}
}
      </code></pre>
    </div>
</section>
-->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>