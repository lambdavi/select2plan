<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval">
  <meta property="og:title" content="Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval"/>
  <meta property="og:description" content="https://lambdavi.github.io/select2plan/"/>
  <meta property="og:url" content="https://lambdavi.github.io/select2plan/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/new_teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval">
  <meta name="twitter:description" content="https://lambdavi.github.io/select2plan/">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/new_teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="select2plan vlm navigation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://lambdavi.github.io" target="_blank">Davide Buoso</a><sup>* 1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://ori.ox.ac.uk/people/luke-robinson" target="_blank">Luke Robinson</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://giuseppeaverta.me" target="_blank">Giuseppe Averta</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://eng.ox.ac.uk/people/philip-torr/" target="_blank">Philip Torr</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://fratim.github.io/" target="_blank">Tim Franzmeyer</a><sup>† 2</sup>,</span>
                    <span class="author-block">
                    <a href="https://ori-mrg.github.io/people/daniele-de-martini/" target="_blank">Daniele De Martini</a><sup>† 2</sup>
                  </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Politecnico di Torino <sup>1</sup>, University of Oxford <sup>2</sup><br> <strong style="color: #3273dc;">Accepted at IEEE RA-L</strong></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author: davide.buoso@polito.it</small></span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Indicates Equal Supervision</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://ieeexplore.ieee.org/document/11151762" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span><strong>IEEE Preprint</strong></span>
                      </a>
                    </span>
                    
                    <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>
                  -->

                  <!-- Github link-->
                  <span class="link-block">
                    <a href="#" 
                    class="external-link button is-normal is-rounded is-dark" style="opacity: 0.6; cursor: not-allowed; pointer-events: none;">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
                
              
              <!-- ArXiv abstract Link  -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.04006" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/new_teaser.png" alt="" style="max-width: 1024px; margin: auto; margin-bottom: 16px;" />
      </figure>
      <h2 class="subtitle has-text-centered">
        We propose the Select2Plan framework, a training-free approach that can be applied in either Third-Person View
        (TPV) or First Person View (FPV) scenarios. It leverages Memory Banks, which are separate for TPV (from in-domain,
        online videos, and human demonstrations) and for FPV (various simulated environments and target objects), to retrieve
        relevant annotated samples. Given a current observation, Select2Plan generates plans (in form of text), which are then
        mapped to actions and used directly for execution. The framework focus is to meet any user's need while mantaining high
        performance and generalization ability.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->
<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1926229700&color=%23654812&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true"></iframe><div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;"><a href="https://soundcloud.com/davide-buoso-429794670" title="Davide Buoso" target="_blank" style="color: #cccccc; text-decoration: none;">Davide Buoso</a> · <a href="https://soundcloud.com/davide-buoso-429794670/select2plan" title="Select2Plan: NotebookLM Podcast" target="_blank" style="color: #cccccc; text-decoration: none;">Select2Plan: NotebookLM Podcast</a></div>
<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This study explores the potential of off-the-shelf Vision-Language Models (VLMs) for high-level robot planning in the context of autonomous navigation. Indeed, while most of existing learning-based approaches for path planning require extensive task-specific training/fine-tuning, we demonstrate how such training can be avoided for most practical cases. To do this, we introduce Select2Plan (S2P), a novel training-free frame work for high-level robot planning which completely eliminates the need for fine-tuning or specialised training. By leveraging structured Visual Question-Answering (VQA) and In-Context Learning (ICL), our approach drastically reduces the need for data collection, requiring a fraction of the task-specific data typically used by trained models, or even relying only on online data. Our method facilitates the effective use of a generally trained VLM in a flexible and cost-efficient way, and does not require additional sensing except for a simple monocular camera. We demonstrate its adaptability across various scene types, context sources, and sensing setups. We evaluate our approach in two distinct scenarios: traditional First-Person View (FPV) and infrastructure-driven Third-Person View (TPV) navigation, demonstrating the flexibility and simplicity of our method. Our technique significantly enhances the navigational capabilities of a baseline VLM of approximately 50% in TPV scenario, and is comparable to trained models in the FPV one, with as few as 20 demonstrations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero" style="padding-bottom: .75rem; padding-top: 1.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-bottom: .75rem; padding-top: 1.5rem;">
      <h3 class="title is-3">Can we plan and navigate in a training-free matter leveraging VQA and ICL?</h3>
      <p>
        <b>Select2Plan (S2P)</b> is a novel framework for high-level robot navigation that eliminates the need for extensive training or fine-tuning. The system leverages <b>off-the-shelf Vision-Language Models (VLMs)</b> combined with <b>Visual Question-Answering (VQA)</b> to make decisions without the need for specialized data collection.
      </p>
      <!--
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/what_can_we_learn.png" alt="" style="max-width: 768px; margin: auto;" />
      </figure>
      -->
      <br>
      <h3 class="title is-3">Can we adapt to different perspectives?</h3>
      <p>
        <b>S2P</b> adapts effortlessy to different navigation scenarios, including <b>First-Person View (FPV)</b> and <b>Third-Person View (TPV)</b>. This versatility enables it to operate in various conditions, from autonomous vehicle pathfinding to CCTV-based robotic control.<br>
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/model_arch.png" alt="" style="margin: auto;" />
      </figure>
      
      <p>
        S2P incorporates <b>In-Context Learning (ICL)</b> combined with <b>experiential memory</b> to generate robust plans.<br>
        The memory retrieval system enables the robot to learn from previous experiences and generalize to new, unseen environments, improving navigation performance by as much as 50% without the need for additional training.
        <b>S2P performs comparably</b> to heavily trained models in traditional setups (FPV), achieving impressive results with significantly fewer data points, making it an efficient solution for high-level robot planning and navigation.
      </p>
      <br>
      <h4 class="title is-4">What matters in the context?</h4>
      <p>
        We also investigate how different data sources can affect the performance of S2P. In TPV scenario, we evaluate the performance of context coming from three different sources: same deployment scenario and same rooms, same scenario, different rooms
        same scenario, humans as demonstrators and finally, different scenario and different robotic support (online data).
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/context_v3.png" alt="" style="max-width: 768px; margin: auto;" />
      </figure>
    </div>
  </div>
</section>

<section class="section hero" style="padding-bottom: .75rem; padding-top: 1.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-bottom: .75rem; padding-top: 1.5rem;">
      <h3 class="title is-3">Results in FPV scenario</h3>
      <p>
        In the FPV setup, our model, referred to as Select2Plan (S2P), was evaluated against
        state-of-the-art methods in a variety of scenarios. The average Success Rate (SR) of
        46.16% in known scenes with known objects reflects the model's ability to perform well
        even with a minimal training dataset. Compared to the best-performing model, which
        was trained on 8 million episodes, S2P required only a fraction of the data, specifically
        one episode per object type (total of 15). Despite this, S2P managed to achieve comparable results,
        highlighting the efficiency of our approach in leveraging pre-trained Vision-Language
        Models (VLMs) for efficient knowledge transfer
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/table1.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
      </figure>

      <h3 class="title is-3">Results in TPV scenario</h3>
      <p>
      Our In-Context Learning (ICL) approach demonstrated
      significant improvements in the TPV scenario. The highest Trajectory Score (TS) achieved
      was 270.70 in context scenario A, which allows unrestricted retrieval from the database,
      compared to the baseline zero-shot approach, which scored 147.82. This represents a
      54.6% improvement, indicating that our model can effectively leverage contextual information to enhance navigational accuracy and safety.
      In addition to the overall TS improvement, the framework showed a remarkable 38%
      reduction in selecting dangerous points (DS), which indicates locations with a high risk of collision. This reduction is crucial for real-world applications, where safety and reliable navigation are paramount. Such performance gains were consistent across different
      context scenarios, further highlighting the versatility and adaptability of the proposed ap-
      proach.
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/table2.png" alt="" style="max-width: 420px; margin: auto; margin-bottom: 16px;" />
      </figure>

      <!--
      <h4 class="title is-4">Step 2: Novel Task Learning</h2>
        <div class="columns is-vcentered is-flex-center is-multiline">
          <div class="column is-half">
            <div class="text-block">
              <p>
                These heads model different and complementary perspectives on the content of the video. 
                We can collect these perspectives in a set of action-wise <b>task-specific prototypes</b>.
                <br><br>
                We call these prototypes a <b>backpack of skills</b> and they represent a frozen snapshot of what the 
                model has learnt in the pre-training phase.
              </p>
            </div>
          </div>
          <div class="column is-half">
            <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
              <img src="static/images/egopack.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
            </figure>
          </div>
          <div class="column is-half">
            <div class="text-block">
              <p>
                <ol>
                  <li>
                    When learning a novel task, we feed the temporal features through the 
                    task-specific heads of the pre-training tasks.
                  </li>
                  <br>
                  <li>
                    These features act as queries to look for the closest matching prototypes 
                    using k-NN in the features space.
                  </li>
                  <br>
                  <li>
                    We refine the task features using <b>Message Passing with task prototypes</b>.
                  </li>
                </ol>
                
              </p>
            </div>
          </div>
            <div class="column is-half">
              <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
                <img src="static/images/fusion.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
              </figure>
            </div>
        </div>
-->

    </div>
  </div>
</section>
<!--
<section class="section hero" style="padding-bottom: .75rem; padding-top: 1.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-bottom: .75rem; padding-top: 1.5rem;">
      <h3 class="title is-3">Experimental results</h3>
      <p>We validate EgoPack on Action Recognition (AR), Object State Change Classification (OSCC), Point of No Return (PNR) and Long Term Anticipation (LTA) from Ego4D.</p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/results_val.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
      </figure>
      <div class="columns is-vcentered is-flex-center is-multiline">
        <div class="column is-half">
          <div class="text-block">
            <h5 class="title is-5">Cross-tasks agreement ratio</h5>
            <p>
              How much different <i>perspectives</i> bring complementary information?
            </p>
          </div>
        </div>
        <div class="column is-half">
          <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
            <img src="static/images/viz_agreement.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
          </figure>
        </div>
        <div class="column is-half">
          <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
            <img src="static/images/queries.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
          </figure>
        </div>
        <div class="column is-half">
          <div class="text-block">
            <h5 class="title is-5" style="text-align: right;">Queried prototypes</h5>
            <p style="text-align: right;">
              When the novel task is OSCC, what are the closest prototypes from the AR and PNR tasks?
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

-->
BibTex citation
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Cite us</h2>
      <pre><code>
        @ARTICLE{11151762,
          author={Buoso, Davide and Robinson, Luke and Averta, Giuseppe and Torr, Philip and Franzmeyer, Tim and De Martini, Daniele},
          journal={IEEE Robotics and Automation Letters}, 
          title={Select2Plan: Training-Free ICL-Based Planning Through VQA and Memory Retrieval}, 
          year={2025},
          volume={},
          number={},
          pages={1-8},
          keywords={Navigation;Robots;Planning;Visualization;Cameras;Training;Adaptation models;Transformers;Predictive models;Oral communication;Motion and Path Planning;Vision-Based Navigation;Autonomous Agents},
          doi={10.1109/LRA.2025.3606790}}
      </code></pre>
    </div>
</section>

<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>